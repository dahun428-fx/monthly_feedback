import os
import json
from typing import Any, Dict, List, Tuple
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold
from .executor import execute_plan
from mcp_server.tools import DESCRIPTIONS

# === Gemini ëª¨ë¸ ì„¤ì • ===
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))

model = genai.GenerativeModel(
    "gemini-2.0-flash",
    generation_config={
        "response_mime_type": "application/json",
        "temperature": 0.2,
        "top_p": 0.9,
        "max_output_tokens": 8192,
    },
    safety_settings={
        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
    },
)

# ê° ë„êµ¬ê°€ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì–´ë–¤ ì¸ìê°€ í•„ìš”í•œì§€ ì •ì˜
TOOL_CONTEXT_MAP: Dict[str, List[str]] = {
    "summarize_text": ["text_to_summarize"],
    "generate_feedback": ["month", "todos", "kpi_summary"],
    "export_to_notion": ["month", "content"],
    "export_report": ["month", "content"],
    # get_today / list_todos / parse_pdf ë“±ì€ ì¸ì ë¶ˆí•„ìš” ë˜ëŠ” ì‹¤í–‰ ê²°ê³¼ë¡œ ì—°ê²°
}

# 1) í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì—ì„œ DESCRIPTIONS ìë¦¬ë§Œ í† í°ìœ¼ë¡œ ë‚¨ê¹ë‹ˆë‹¤.
SYSTEM_PROMPT_CORE = """ë‹¹ì‹ ì€ ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ì—¬ ì‚¬ìš©ìë¥¼ ë•ëŠ” AI ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤. ëª©í‘œëŠ” ì‚¬ìš©ìì˜ ìš”ì²­ì„ í•´ê²°í•˜ê¸° ìœ„í•´
í•œ ë²ˆì— 'ì •í™•íˆ í•˜ë‚˜'ì˜ ë„êµ¬ë§Œ í˜¸ì¶œí•˜ê±°ë‚˜(final_answer) ë‹¤ìŒ ë‹¨ê³„ ê²°ë¡ ì„ ë°˜í™˜í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡:
{DESCRIPTIONS}

[ì‹œë‚˜ë¦¬ì˜¤ ì˜ˆì‹œ: ì›”ê°„ ë³´ê³ ì„œ + Notion]
1) get_today â†’ 2) list_todos â†’ 3) get_pdf_filename â†’ 4) parse_pdf â†’ 5) summarize_text â†’ 6) generate_feedback â†’ 7) export_to_notion
í•­ìƒ 'í•œ ë‹¨ê³„ì”©ë§Œ' ì§„í–‰í•˜ì„¸ìš”.

[ì‘ë‹µ ê·œì¹™]
- í•­ìƒ JSONìœ¼ë¡œë§Œ ì‘ë‹µ.
- ì›”ê°„ ë³´ê³ ì„œ ì‘ì—… ì‹œ list_todos íˆ´ì„ ì‚¬ìš© í• ë•ŒëŠ”, get_today íˆ´ë¡œë¶€í„° ë°›ì€ date ì¤‘ ë™ì¼í•œ month ì— í•´ë‹¹í•˜ëŠ” todo ëª©ë¡ì„ ì¡°íšŒí•˜ì„¸ìš”.
- ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì˜ ìµœìƒìœ„ í‚¤ë¥¼ í¬í•¨:
  - tool_code: {"tool": string, "args": object}
  - final_answer: string
- í•˜ë‚˜ì˜ ì‘ì—…ì´ `final_answer`ë¡œ ì™„ë£Œë˜ë©´, ê·¸ ì‘ì—…ì€ ëë‚œ ê²ƒì…ë‹ˆë‹¤. ë‹¤ìŒ ì‚¬ìš©ì ë©”ì‹œì§€ëŠ” ì´ì „ ëŒ€í™”ì™€ ê´€ë ¨ ì—†ëŠ” **ìƒˆë¡œìš´ ìš”ì²­**ìœ¼ë¡œ ê°„ì£¼í•´ì•¼ í•©ë‹ˆë‹¤. (ë‹¨, ì‚¬ìš©ìê°€ ì´ì „ ê²°ê³¼ì— ëŒ€í•´ ì§ì ‘ ì§ˆë¬¸í•˜ëŠ” ê²½ìš°ëŠ” ì˜ˆì™¸ì…ë‹ˆë‹¤.)
- í•  ì¼ ëª©ë¡(todos)ì„ ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤„ ë•ŒëŠ”, ê° í•­ëª©ì„ ê¸€ë¨¸ë¦¬ ê¸°í˜¸(-)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‚ ì§œ, í•  ì¼, ìƒíƒœ ìˆœì„œë¡œ ë³´ê¸° ì¢‹ê²Œ ì •ë¦¬í•´ì„œ ë³´ì—¬ì£¼ì„¸ìš”.
- ë„êµ¬ í˜¸ì¶œ ì‹¤íŒ¨ í”¼ë“œë°±ì„ ë°›ìœ¼ë©´, ì›ì¸ í•´ê²°ì„ ìœ„í•œ 'ë‹¤ìŒ ë‹¨ì¼ ë„êµ¬'ë¥¼ ì œì•ˆí•˜ì„¸ìš”.
- ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë„êµ¬ëŠ” ì ˆëŒ€ ë§Œë“¤ì§€ ë§ˆì„¸ìš”.
"""

def get_system_prompt(command: str = "") -> str:
    # 2) .format() ëŒ€ì‹  ì •í™•íˆ ì´ í† í°ë§Œ ì¹˜í™˜
    core = SYSTEM_PROMPT_CORE.replace("{DESCRIPTIONS}", DESCRIPTIONS)
    return f"{core}\nì‚¬ìš©ì ëª…ë ¹ì–´: {command}\n"

def _merge_tool_result_into_context(context: Dict[str, Any], result_data: Dict[str, Any]) -> None:
    """
    ì œë„¤ë¦­ ë¨¸ì§€: ê²°ê³¼ë¡œ ë“¤ì–´ì˜¨ í‚¤ë¥¼ ì»¨í…ìŠ¤íŠ¸ì— ì•ˆì „í•˜ê²Œ ë°˜ì˜.
    - íƒ€ì…ì„ ë³€í˜•í•˜ì§€ ì•ŠìŒ(ë¬¸ìì—´í™” ê¸ˆì§€)
    - íŒŒìƒ í•„ë“œ(month ë“±) ê³„ì‚°
    """
    # ì›ë³¸ í‚¤ë“¤ ê·¸ëŒ€ë¡œ ë°˜ì˜
    for k, v in result_data.items():
        context[k] = v

    # íŒŒìƒ: today â†’ month
    if "today" in result_data and isinstance(result_data["today"], str) and len(result_data["today"]) >= 7:
        context["month"] = result_data["today"][:7]

    # íŒŒìƒ: parse_pdf â†’ summarize_text ì…ë ¥ ì¤€ë¹„
    if "text" in result_data and isinstance(result_data["text"], str):
        context["text_to_summarize"] = result_data["text"]

    # íŒŒìƒ: summarize_text â†’ generate_feedback ì…ë ¥ ì¤€ë¹„
    if "summary" in result_data and isinstance(result_data["summary"], str):
        context["kpi_summary"] = result_data["summary"]

    # íŒŒìƒ: generate_feedback â†’ export ê³„ì—´ ì…ë ¥ ì¤€ë¹„
    if "content" in result_data and isinstance(result_data["content"], str):
        context["content"] = result_data["content"]

def _inject_args_from_context(tool: str, args: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:
    if tool in TOOL_CONTEXT_MAP:
        for name in TOOL_CONTEXT_MAP[tool]:
            if name in context and name not in args:
                args[name] = context[name]
    return args

def _as_user_feedback(tool: str, ok: bool, payload: Dict[str, Any]) -> str:
    if ok:
        # ìš”ì•½ í—¤ë” + ì¶•ì•½ JSON(ê¸¸ë©´ ì˜ë¼ì„œ)
        short = json.dumps(payload, ensure_ascii=False)
        if len(short) > 2000:  # í† í° ì ˆì•½
            short = short[:2000] + "â€¦(truncated)"
        return f"Tool {tool} executed successfully.\nResult JSON:\n{short}"
    else:
        message = payload.get("message", "Unknown error")
        return f"Tool {tool} failed.\nReason: {message}\nHint: Provide missing args or call a preparatory tool."

def agent_step(messages: List[Dict[str, Any]], context: Dict[str, Any]) -> Tuple[List[Dict[str, Any]], Dict[str, Any], str, bool, Dict[str, Any] | None]:
    """
    ì—ì´ì „íŠ¸ì˜ ë‹¨ì¼ ìŠ¤í….
    """
    wip_content = None
    try:
        response = model.generate_content(messages)
        text = getattr(response, "text", "") or ""
    except Exception as e:
        ui_message = f"ëª¨ë¸ í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜: {e}"
        return messages, context, ui_message, True, None

    # JSON íŒŒì‹±
    try:
        llm_response = json.loads(text)
        print(f"ğŸ’¡ LLM says: {json.dumps(llm_response, indent=2, ensure_ascii=False)}")
    except Exception as e:
        print(f"âŒ JSON decode error: {e}\nRaw: {text}")
        llm_response = {"final_answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}

    # tool_code ê²½ë¡œ
    if "tool_code" in llm_response:
        tool_call = llm_response["tool_code"]
        if isinstance(tool_call, str):
            tool_call = {"tool": tool_call, "args": {}}

        tool_name = tool_call.get("tool")
        tool_args = tool_call.get("args", {}) or {}

        # ì»¨í…ìŠ¤íŠ¸ì—ì„œ í•„ìš”í•œ ì¸ì ì£¼ì…
        tool_args = _inject_args_from_context(tool_name, tool_args, context)
        tool_call["args"] = tool_args

        # íˆìŠ¤í† ë¦¬ì— 'ëª¨ë¸ ì˜ë„' ê¸°ë¡
        messages.append({"role": "model", "parts": [{"text": json.dumps({"tool_code": tool_call}, ensure_ascii=False)}]})

        # ì‹¤ì œ ë„êµ¬ ì‹¤í–‰
        execution_result = execute_plan(tool_call)
        status_ok = (execution_result.get("status") == "200")
        result_payload = execution_result.get("result", {}) if isinstance(execution_result, dict) else {}

        # ì»¨í…ìŠ¤íŠ¸ ë³‘í•©(ì„±ê³µ ì‹œ)
        if status_ok and isinstance(result_payload, dict) and result_payload.get("status") != "error":
            _merge_tool_result_into_context(context, result_payload)
            feedback = _as_user_feedback(tool_name, True, result_payload)
            wip_content = result_payload
        else:
            # ì‹¤íŒ¨ ì¼€ì´ìŠ¤
            err_payload = result_payload if isinstance(result_payload, dict) else {"message": execution_result}
            feedback = _as_user_feedback(tool_name, False, err_payload)

        # LLMì—ê²Œ ê²°ê³¼ ì „ë‹¬
        messages.append({"role": "user", "parts": [{"text": feedback}]})
        return messages, context, f"ğŸ› ï¸ {tool_name} ì‹¤í–‰", False, wip_content

    # final_answer ê²½ë¡œ
    if "final_answer" in llm_response:
        return messages, context, llm_response["final_answer"], True, None

    # ê²°ì • ì‹¤íŒ¨
    return messages, context, "ì—ì´ì „íŠ¸ê°€ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê²°ì •í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë£¨í”„ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.", True, None

def run_agent(command: str, *, max_steps: int = 20) -> None:
    """
    ì—ì´ì „íŠ¸ë¥¼ ì‹¤í–‰í•˜ì—¬ ìµœì¢… ë‹µë³€ì— ë„ë‹¬í•  ë•Œê¹Œì§€ ë°˜ë³µ.
    """
    print(f"ğŸš€ Starting agent with command: {command}")

    messages: List[Dict[str, Any]] = [
        {"role": "system", "parts": [{"text": get_system_prompt(command)}]},
        {"role": "user", "parts": [{"text": command}]},
    ]
    context: Dict[str, Any] = {}

    last_tool: str | None = None
    same_tool_count = 0

    for step in range(1, max_steps + 1):
        print(f"\nğŸ¤” Step {step}â€¦")
        messages, context, ui_message, is_final, wip_content = agent_step(messages, context)
        print(f"âœ… Agent step result: {ui_message}")
        if wip_content:
            print(f"  - Work In Progress: {json.dumps(wip_content, indent=2, ensure_ascii=False)}")


        # ë‹¨ìˆœí•œ ë¬´í•œ ë°˜ë³µ ë°©ì§€: ê°™ì€ ë„êµ¬ ì—°ì† í˜¸ì¶œ ê°ì§€ (ì„ íƒ)
        if len(messages) >= 2 and messages[-2]["role"] == "model":
            try:
                last_intent = json.loads(messages[-2]["parts"][0]["text"])
                cur_tool = last_intent.get("tool_code", {}).get("tool")
                if cur_tool and cur_tool == last_tool:
                    same_tool_count += 1
                else:
                    same_tool_count = 0
                last_tool = cur_tool
                if same_tool_count >= 3:
                    print("âš ï¸ ë™ì¼ ë„êµ¬ë¥¼ ë°˜ë³µ í˜¸ì¶œí•˜ì—¬ ë£¨í”„ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break
            except Exception:
                pass

        if is_final:
            print(f"\nğŸ Final Answer: {ui_message}")
            return

    print("\nâ¹ï¸ ìµœëŒ€ ìŠ¤í…ì— ë„ë‹¬í•˜ì—¬ ì¢…ë£Œí–ˆìŠµë‹ˆë‹¤.")
