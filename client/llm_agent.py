import os
import re
import json
import google.generativeai as genai
from .executor import execute_plan
from mcp_server.tools import DESCRIPTIONS

genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
model = genai.GenerativeModel(
    "gemini-1.5-flash",
    generation_config={"response_mime_type": "application/json"},
)

# ê° ë„êµ¬ê°€ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì–´ë–¤ ì¸ìë¥¼ í•„ìš”ë¡œ í•˜ëŠ”ì§€ ëª…ì‹œí•˜ëŠ” ë§µ
TOOL_CONTEXT_MAP = {
    "summarize_text": ["text_to_summarize"],
    "generate_feedback": ["month", "todos", "kpi_summary"],
    "export_to_notion": ["month", "content"],
    "export_report": ["month", "content"],
}

def run_agent(command: str):
    """
    Runs the agent loop to process a command, using tools until a final answer is reached.
    """
    print(f"ğŸš€ Starting agent with command: {command}")

    messages = [
        {
            "role": "user",
            "parts": [
                {
                    "text": f"""ë‹¹ì‹ ì€ ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ì—¬ ì‚¬ìš©ìë¥¼ ë•ëŠ” AI ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ëª©í‘œëŠ” ì‚¬ìš©ìì˜ ìš”ì²­ì— ê°€ì¥ ì ì ˆí•œ ë‹¨ í•˜ë‚˜ì˜ ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ì—¬ ì‘ë‹µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡:
{DESCRIPTIONS}

# ì£¼ìš” ì‘ì—… ì‹œë‚˜ë¦¬ì˜¤
- **ì›”ê°„ ë³´ê³ ì„œ ìƒì„± ë° Notion ë‚´ë³´ë‚´ê¸°**: ì´ ìš”ì²­ì„ ë°›ìœ¼ë©´, ë°˜ë“œì‹œ ì•„ë˜ ìˆœì„œëŒ€ë¡œ ë„êµ¬ë¥¼ í˜¸ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.
  1. `get_today` (ì˜¤ëŠ˜ ë‚ ì§œ í™•ì¸)
  2. `list_todos` (í•  ì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°)
  3. `list_pdf_files` (PDF íŒŒì¼ ëª©ë¡ í™•ì¸)
  4. `parse_pdf` (PDF ë‚´ìš© ì¶”ì¶œ)
  5. `summarize_text` (ì¶”ì¶œëœ ë‚´ìš© ìš”ì•½)
  6. `generate_feedback` (ëª¨ë“  ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ë³´ê³ ì„œ ë‚´ìš© ìƒì„±)
  7. `export_to_notion` (ìƒì„±ëœ ë³´ê³ ì„œ ë‚´ìš©ì„ Notionìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°)

# ê·œì¹™
- ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì•¼ í•©ë‹ˆë‹¤.
- ì‘ë‹µì€ ë‹¨ì¼ ë„êµ¬ í˜¸ì¶œì„ ìœ„í•œ 'tool_code' í•„ë“œ ë˜ëŠ” ìµœì¢… ë‹µë³€ì„ ìœ„í•œ 'final_answer' í•„ë“œë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
- **ì¤‘ìš”**: ì—¬ëŸ¬ ë‹¨ê³„ë¥¼ ë¯¸ë¦¬ ê³„íší•˜ì§€ ë§ˆì„¸ìš”. ì‹œë‚˜ë¦¬ì˜¤ë‚˜ ì‚¬ìš©ìì˜ ë§ˆì§€ë§‰ ë©”ì‹œì§€ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ë‹¤ìŒ ë‹¨ê³„ë§Œ ìƒê°í•˜ê³ , ê°€ì¥ ì§ì ‘ì ìœ¼ë¡œ ë„ì›€ì´ ë˜ëŠ” ë„êµ¬ í•˜ë‚˜ë§Œ ì„ íƒí•˜ì„¸ìš”.
- **ì˜¤ë¥˜ ì²˜ë¦¬**: ë„êµ¬ í˜¸ì¶œì´ ì‹¤íŒ¨í–ˆë‹¤ëŠ” ë©”ì‹œì§€ë¥¼ ë°›ìœ¼ë©´, ê·¸ ì›ì¸ì„ í•´ê²°í•˜ê¸° ìœ„í•œ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ìƒê°í•˜ì„¸ìš”. ì˜ˆë¥¼ ë“¤ì–´, ì¸ìê°€ ë¶€ì¡±í•´ì„œ ì‹¤íŒ¨í–ˆë‹¤ë©´, ê·¸ ì¸ìë¥¼ ì–»ì„ ìˆ˜ ìˆëŠ” ë‹¤ë¥¸ ë„êµ¬ë¥¼ í˜¸ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.
- ì—†ëŠ” ë„êµ¬ë¥¼ ë§Œë“¤ì§€ ë§ê³ , ì£¼ì–´ì§„ ë„êµ¬ë§Œ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.

ì‚¬ìš©ì ëª…ë ¹ì–´: {command}
"""
                }
            ],
        }
    ]
    
    context = {}

    while True:
        print("\nğŸ¤” Thinking...")
        response = model.generate_content(messages)
        
        try:
            llm_response = json.loads(response.text)
            print(f"ğŸ’¡ LLM says: {json.dumps(llm_response, indent=2, ensure_ascii=False)}")
        except (json.JSONDecodeError, AttributeError) as e:
            print(f"âŒ Error decoding LLM response: {e}")
            print(f"Raw response: {response.text}")
            llm_response = {"final_answer": "ì£„ì†¡í•©ë‹ˆë‹¤, ì‘ë‹µì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}

        if "tool_code" in llm_response:
            tool_call = llm_response["tool_code"]
            
            if isinstance(tool_call, str):
                tool_call = {"tool": tool_call, "args": {}}

            tool_name = tool_call.get("tool")
            tool_args = tool_call.get("args", {})

            # ğŸ§  ë„êµ¬ì— í•„ìš”í•œ ì¸ìë§Œ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì„ ë³„ì ìœ¼ë¡œ ì£¼ì… (ì»¨í…ìŠ¤íŠ¸ ê°’ì„ ìš°ì„ )
            if tool_name in TOOL_CONTEXT_MAP:
                for arg_name in TOOL_CONTEXT_MAP[tool_name]:
                    if arg_name in context:
                        tool_args[arg_name] = context[arg_name]
            tool_call["args"] = tool_args

            messages.append({"role": "model", "parts": [{"text": json.dumps({"tool_code": tool_call})}]})

            print(f"ğŸ› ï¸  Executing tool: {tool_name} with args: {tool_args}")
            execution_result = execute_plan(tool_call)
            print(f"âœ… Tool result: {json.dumps(execution_result, indent=2, ensure_ascii=False)}")

            # ğŸ§  ì‹¤í–‰ ê²°ê³¼ ì²˜ë¦¬ ë° LLMì—ê²Œ ì „ë‹¬í•  ë©”ì‹œì§€ ìƒì„±
            user_feedback_message = f"Tool {tool_name} executed. Result is stored in context."
            if execution_result.get("status") == "200":
                result_data = execution_result.get("result", {})
                if result_data.get("status") != "error":
                    if "today" in result_data: 
                        context["month"] = result_data["today"][:7]
                        user_feedback_message = f"Tool {tool_name} executed. Result: {json.dumps(result_data)}"
                    if "todos" in result_data: context["todos"] = json.dumps(result_data["todos"], ensure_ascii=False)
                    if "text" in result_data: context["text_to_summarize"] = result_data["text"]
                    if "summary" in result_data: context["kpi_summary"] = result_data["summary"]
                    if "content" in result_data: context["content"] = result_data["content"]
                else:
                    user_feedback_message = f"Tool {tool_name} failed. Error: {result_data.get('message')}"

            messages.append({
                "role": "user",
                "parts": [{ "text": user_feedback_message }]
            })

        elif "final_answer" in llm_response:
            final_answer = llm_response["final_answer"]
            print(f"\nğŸ Final Answer: {final_answer}")
            break
        
        else:
            print("âŒ LLM response did not contain 'tool_code' or 'final_answer'.")
            print("Ending agent loop.")
            break